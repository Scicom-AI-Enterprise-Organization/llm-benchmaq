{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2f9cb535",
      "metadata": {},
      "outputs": [],
      "source": [
        "import benchmaq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f43cc39d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================\n",
            "STEP 1: DEPLOYING RUNPOD POD\n",
            "================================================================\n",
            "Using on-demand instance\n",
            "\n",
            "================================================================\n",
            "STEP 1: DEPLOYING RUNPOD POD\n",
            "================================================================\n",
            "Using on-demand instance\n",
            "\n",
            "================================================================\n",
            "STEP 1: DEPLOYING RUNPOD POD\n",
            "================================================================\n",
            "Using on-demand instance\n",
            "\n",
            "================================================================\n",
            "STEP 1: DEPLOYING RUNPOD POD\n",
            "================================================================\n",
            "Using on-demand instance\n",
            "\n",
            "================================================================\n",
            "STEP 1: DEPLOYING RUNPOD POD\n",
            "================================================================\n",
            "Using on-demand instance\n",
            "\n",
            "================================================================\n",
            "STEP 1: DEPLOYING RUNPOD POD\n",
            "================================================================\n",
            "Using on-demand instance\n",
            "\n",
            "================================================================\n",
            "ERROR: There are no longer any instances available with the requested specifications. Please refresh and try again.\n",
            "================================================================\n",
            "\n",
            "================================================================\n",
            "ERROR: There are no longer any instances available with the requested specifications. Please refresh and try again.\n",
            "================================================================\n",
            "\n",
            "================================================================\n",
            "ERROR: Something went wrong. Please try again later or contact support.\n",
            "================================================================\n",
            "\n",
            "================================================================\n",
            "ERROR: Something went wrong. Please try again later or contact support.\n",
            "================================================================\n",
            "Pod created: nxfesckwflv6sa\n",
            "Pod created: d6mgqvizuwmt0u\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pod ready!:   3%|▎         | 2/60 [00:25<12:18, 12.73s/it]                        \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Done!\n",
            "  SSH: ssh root@82.221.170.234 -p 58116 -i /Users/ariff.a/.ssh/id_ed25519\n",
            "\n",
            "Pod deployed: d6mgqvizuwmt0u\n",
            "Pod name: benchmark_qwen3_32b_h200sxm\n",
            "SSH: ssh root@82.221.170.234 -p 58116 -i /Users/ariff.a/.ssh/id_ed25519\n",
            "\n",
            "================================================================\n",
            "STEP 2: RUNNING BENCHMARKS\n",
            "================================================================\n",
            "Connecting to remote server: root@82.221.170.234:58116\n",
            "Using SSH key: /Users/ariff.a/.ssh/id_ed25519\n",
            "UV environment: ~/.venv (Python 3.11)\n",
            "Dependencies: ['pyyaml', 'requests', 'vllm==0.14.1', 'huggingface_hub']\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pod ready!:   5%|▌         | 3/60 [00:35<11:12, 11.80s/it]                       \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Done!\n",
            "  SSH: ssh root@198.13.252.84 -p 23273 -i /Users/ariff.a/.ssh/id_ed25519\n",
            "\n",
            "Pod deployed: nxfesckwflv6sa\n",
            "Pod name: benchmark_gpt_oss_120b_b200\n",
            "SSH: ssh root@198.13.252.84 -p 23273 -i /Users/ariff.a/.ssh/id_ed25519\n",
            "\n",
            "================================================================\n",
            "STEP 2: RUNNING BENCHMARKS\n",
            "================================================================\n",
            "Connecting to remote server: root@198.13.252.84:23273\n",
            "Using SSH key: /Users/ariff.a/.ssh/id_ed25519\n",
            "UV environment: ~/.venv (Python 3.11)\n",
            "Dependencies: ['pyyaml', 'requests', 'vllm==0.14.1', 'huggingface_hub']\n",
            "\n",
            "\n",
            "================================================================\n",
            "RUN: benchmark_gpt_oss_120b_b200_run_1 | TP=8 DP=1 PP=1\n",
            "================================================================\n",
            "Starting vLLM server: vllm serve Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1 --port 8000 --tensor-parallel-size 8 --pipeline-parallel-size 1 --gpu-memory-utilization 0.9 --max-model-len 16000 --max-num-seqs 256 --dtype bfloat16 --disable-log-requests\n",
            "Waiting for server at http://localhost:8000/health...\n",
            "Health check attempt 1...\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:08\u001b[0m \u001b[90m[api_server.py:1272]\u001b[0m vLLM API server version 0.14.1\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:08\u001b[0m \u001b[90m[utils.py:263]\u001b[0m non-default args: {'model_tag': 'Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1', 'model': 'Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1', 'dtype': 'bfloat16', 'max_model_len': 16000, 'tensor_parallel_size': 8, 'max_num_seqs': 256}\n",
            "config.json: 1.90kB [00:00, 3.37MB/s]\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[model.py:530]\u001b[0m Resolved architecture: GptOssForCausalLM\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[model.py:1545]\u001b[0m Using max model len 16000\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[scheduler.py:229]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[config.py:314]\u001b[0m Overriding max cuda graph capture size to 1024 for performance.\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[vllm.py:630]\u001b[0m Asynchronous scheduling is enabled.\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[vllm.py:637]\u001b[0m Disabling NCCL for DP synchronization when using async scheduling.\n",
            "tokenizer_config.json: 4.20kB [00:00, 8.08MB/s]\n",
            "tokenizer.json: 100%|██████████████████████| 27.9M/27.9M [00:00<00:00, 44.9MB/s]\n",
            "special_tokens_map.json: 100%|█████████████████| 440/440 [00:00<00:00, 1.83MB/s]\n",
            "chat_template.jinja: 16.7kB [00:00, 21.3MB/s]\n",
            "generation_config.json: 100%|███████████████████| 160/160 [00:00<00:00, 621kB/s]\n",
            "\u001b[0;36m(EngineCore_DP0 pid=1007)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:28\u001b[0m \u001b[90m[core.py:97]\u001b[0m Initializing a V1 LLM engine (v0.14.1) with config: model='Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1', speculative_config=None, tokenizer='Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16000, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512, 528, 544, 560, 576, 592, 608, 624, 640, 656, 672, 688, 704, 720, 736, 752, 768, 784, 800, 816, 832, 848, 864, 880, 896, 912, 928, 944, 960, 976, 992, 1008, 1024], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 1024, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=1007)\u001b[0;0m \u001b[33mWARNING\u001b[0m \u001b[90m01-28 03:57:28\u001b[0m \u001b[90m[multiproc_executor.py:880]\u001b[0m Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
            "Health check attempt 11...\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:48\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=4 local_rank=4 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=5 local_rank=5 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=7 local_rank=7 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=6 local_rank=6 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[pynccl.py:111]\u001b[0m vLLM is using nccl==2.27.5\n",
            "Health check attempt 21...\n",
            "Health check attempt 31...\n",
            "Health check attempt 41...\n",
            "Health check attempt 51...\n",
            "Health check attempt 61...\n",
            "Health check attempt 71...\n",
            "Health check attempt 81...\n",
            "Health check attempt 91...\n",
            "Health check attempt 101...\n",
            "Health check attempt 111...\n",
            "Health check attempt 121...\n",
            "\n",
            "================================================================\n",
            "ERROR: Failed to get result from remote.\n",
            "stdout: \n",
            "================================================================\n",
            "RUN: benchmark_gpt_oss_120b_b200_run_1 | TP=8 DP=1 PP=1\n",
            "================================================================\n",
            "Starting vLLM server: vllm serve Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1 --port 8000 --tensor-parallel-size 8 --pipeline-parallel-size 1 --gpu-memory-utilization 0.9 --max-model-len 16000 --max-num-seqs 256 --dtype bfloat16 --disable-log-requests\n",
            "Waiting for server at http://localhost:8000/health...\n",
            "Health check attempt 1...\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:08\u001b[0m \u001b[90m[api_server.py:1272]\u001b[0m vLLM API server version 0.14.1\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:08\u001b[0m \u001b[90m[utils.py:263]\u001b[0m non-default args: {'model_tag': 'Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1', 'model': 'Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1', 'dtype': 'bfloat16', 'max_model_len': 16000, 'tensor_parallel_size': 8, 'max_num_seqs': 256}\n",
            "config.json: 1.90kB [00:00, 3.37MB/s]\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[model.py:530]\u001b[0m Resolved architecture: GptOssForCausalLM\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[model.py:1545]\u001b[0m Using max model len 16000\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[scheduler.py:229]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[config.py:314]\u001b[0m Overriding max cuda graph capture size to 1024 for performance.\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[vllm.py:630]\u001b[0m Asynchronous scheduling is enabled.\n",
            "\u001b[0;36m(APIServer pid=705)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:17\u001b[0m \u001b[90m[vllm.py:637]\u001b[0m Disabling NCCL for DP synchronization when using async scheduling.\n",
            "tokenizer_config.json: 4.20kB [00:00, 8.08MB/s]\n",
            "tokenizer.json: 100%|██████████████████████| 27.9M/27.9M [00:00<00:00, 44.9MB/s]\n",
            "special_tokens_map.json: 100%|█████████████████| 440/440 [00:00<00:00, 1.83MB/s]\n",
            "chat_template.jinja: 16.7kB [00:00, 21.3MB/s]\n",
            "generation_config.json: 100%|███████████████████| 160/160 [00:00<00:00, 621kB/s]\n",
            "\u001b[0;36m(EngineCore_DP0 pid=1007)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:28\u001b[0m \u001b[90m[core.py:97]\u001b[0m Initializing a V1 LLM engine (v0.14.1) with config: model='Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1', speculative_config=None, tokenizer='Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16000, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512, 528, 544, 560, 576, 592, 608, 624, 640, 656, 672, 688, 704, 720, 736, 752, 768, 784, 800, 816, 832, 848, 864, 880, 896, 912, 928, 944, 960, 976, 992, 1008, 1024], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 1024, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=1007)\u001b[0;0m \u001b[33mWARNING\u001b[0m \u001b[90m01-28 03:57:28\u001b[0m \u001b[90m[multiproc_executor.py:880]\u001b[0m Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
            "Health check attempt 11...\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:48\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=4 local_rank=4 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=5 local_rank=5 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=7 local_rank=7 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=8 rank=6 local_rank=6 distributed_init_method=tcp://127.0.0.1:55399 backend=nccl\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-28 03:57:49\u001b[0m \u001b[90m[pynccl.py:111]\u001b[0m vLLM is using nccl==2.27.5\n",
            "Health check attempt 21...\n",
            "Health check attempt 31...\n",
            "Health check attempt 41...\n",
            "Health check attempt 51...\n",
            "Health check attempt 61...\n",
            "Health check attempt 71...\n",
            "Health check attempt 81...\n",
            "Health check attempt 91...\n",
            "Health check attempt 101...\n",
            "Health check attempt 111...\n",
            "Health check attempt 121...\n",
            "\n",
            "stderr: \n",
            "================================================================\n",
            "\n",
            "Deleting pod: nxfesckwflv6sa\n",
            "Pod deleted: {'status': 'deleted', 'id': 'nxfesckwflv6sa', 'name': None}\n",
            "\n",
            "================================================================\n",
            "ERROR: Failed to install dependencies ['pyyaml', 'requests', 'vllm==0.14.1', 'huggingface_hub']: Resolved 162 packages in 10.33s\n",
            "Downloading nvidia-cudnn-cu12 (674.0MiB)\n",
            "Downloading nvidia-cusolver-cu12 (255.1MiB)\n",
            "Downloading nvidia-cufft-cu12 (184.2MiB)\n",
            "Downloading nvidia-cuda-nvrtc-cu12 (84.0MiB)\n",
            "Downloading mistral-common (6.2MiB)\n",
            "Downloading transformers (11.4MiB)\n",
            "Downloading nvidia-cuda-cupti-cu12 (9.8MiB)\n",
            "Downloading torchvision (7.7MiB)\n",
            "Downloading pillow (6.7MiB)\n",
            "Downloading grpcio (6.3MiB)\n",
            "Downloading pycountry (6.0MiB)\n",
            "Downloading sympy (6.0MiB)\n",
            "Downloading cryptography (4.2MiB)\n",
            "Downloading numba (3.7MiB)\n",
            "Downloading hf-xet (3.2MiB)\n",
            "Downloading tokenizers (3.1MiB)\n",
            "Downloading llguidance (2.9MiB)\n",
            "Downloading openai-harmony (2.8MiB)\n",
            "Downloading networkx (2.0MiB)\n",
            "Downloading pydantic-core (2.0MiB)\n",
            "Downloading torchaudio (2.0MiB)\n",
            "Downloading apache-tvm-ffi (2.0MiB)\n",
            "Downloading aiohttp (1.7MiB)\n",
            "Downloading sentencepiece (1.3MiB)\n",
            "Downloading pygments (1.2MiB)\n",
            "Downloading nvidia-cufile-cu12 (1.1MiB)\n",
            "Downloading openai (1.0MiB)\n",
            "Downloading uvloop (3.6MiB)\n",
            "Downloading nvidia-cudnn-frontend (2.1MiB)\n",
            "Downloading numpy (16.0MiB)\n",
            "Downloading torch (858.1MiB)\n",
            "Downloading nvidia-cusparselt-cu12 (273.9MiB)\n",
            "Downloading triton (162.5MiB)\n",
            "Downloading vllm (472.4MiB)\n",
            "Downloading ray (69.0MiB)\n",
            "Downloading cupy-cuda12x (107.8MiB)\n",
            "Downloading nvidia-nvjitlink-cu12 (37.4MiB)\n",
            "Downloading nvidia-nccl-cu12 (307.4MiB)\n",
            "Downloading tiktoken (1.1MiB)\n",
            "Downloading opencv-python-headless (59.6MiB)\n",
            "Downloading outlines-core (2.2MiB)\n",
            "Downloading xgrammar (33.3MiB)\n",
            "Downloading nvidia-nvshmem-cu12 (118.9MiB)\n",
            "Downloading llvmlite (40.4MiB)\n",
            "Downloading cuda-bindings (15.6MiB)\n",
            "Downloading nvidia-cutlass-dsl (55.9MiB)\n",
            "Downloading nvidia-curand-cu12 (60.7MiB)\n",
            "Downloading nvidia-cusparse-cu12 (274.9MiB)\n",
            "Downloading nvidia-cublas-cu12 (566.8MiB)\n",
            "Downloading flashinfer-python (6.7MiB)\n",
            " Downloaded openai\n",
            "Downloading setuptools (1.0MiB)\n",
            " Downloaded setuptools\n",
            " Downloaded nvidia-cufile-cu12\n",
            " Downloaded pygments\n",
            " Downloaded tiktoken\n",
            " Downloaded sentencepiece\n",
            " Downloaded aiohttp\n",
            " Downloaded torchaudio\n",
            " Downloaded pydantic-core\n",
            " Downloaded apache-tvm-ffi\n",
            " Downloaded networkx\n",
            " Downloaded nvidia-cudnn-frontend\n",
            " Downloaded outlines-core\n",
            " Downloaded openai-harmony\n",
            " Downloaded llguidance\n",
            " Downloaded hf-xet\n",
            " Downloaded tokenizers\n",
            " Downloaded uvloop\n",
            " Downloaded numba\n",
            " Downloaded cryptography\n",
            " Downloaded sympy\n",
            " Downloaded pycountry\n",
            " Downloaded mistral-common\n",
            " Downloaded grpcio\n",
            " Downloaded pillow\n",
            " Downloaded flashinfer-python\n",
            " Downloaded torchvision\n",
            " Downloaded nvidia-cuda-cupti-cu12\n",
            " Downloaded transformers\n",
            " Downloaded cuda-bindings\n",
            " Downloaded numpy\n",
            " Downloaded xgrammar\n",
            "\n",
            "================================================================\n",
            "\n",
            "Deleting pod: d6mgqvizuwmt0u\n",
            "Pod deleted: {'status': 'deleted', 'id': 'd6mgqvizuwmt0u', 'name': None}\n"
          ]
        }
      ],
      "source": [
        "from multiprocessing import Pool\n",
        "from benchmaq.runpod import run_benchmark  # Multiprocessing-compatible function\n",
        "\n",
        "configs = [\n",
        "    \"b200-gpt-oss-120b.yaml\",  \n",
        "    \"b200-qwen3-32b.yaml\",  \n",
        "    \"h100sxm-gpt-oss-120b.yaml\",  \n",
        "    \"h100sxm-qwen3-32b.yaml\",  \n",
        "    \"h200sxm-gpt-oss-120b.yaml\",    \n",
        "    \"h200sxm-qwen3-32b.yaml\",\n",
        "]\n",
        "\n",
        "# Run next config (b200-qwen3-32b.yaml)\n",
        "benchmaq.runpod.run_benchmark(configs[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1712f9b9",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
