# =============================================================================
# Remote Benchmark via SSH (Password Authentication)
# =============================================================================
# Run benchmarks on a remote GPU server using SSH with password auth.
# Usage: benchmaxxing bench examples/3_remote_gpu_ssh_password.yaml
#

## Remote Connection
## -------------------------------------------------------------------------
## SSH connection settings for the remote GPU server.
##
remote:
  # -- Host address or IP of the remote server
  host: "gpu-server.example.com"
  # -- SSH username
  username: "ubuntu"
  # -- SSH password for authentication
  password: "your-password-here"

  ## Python Environment
  ## -----------------------------------------------------------------------
  ## UV virtual environment configuration on the remote server.
  ##
  uv:
    # -- Path to virtual environment
    path: "~/.benchmark-venv"
    # -- Python version to use
    python_version: "3.11"

  ## Dependencies
  ## -----------------------------------------------------------------------
  ## Packages to install in the remote environment.
  ##
  dependencies:
    - vllm==0.11.0
    - pyyaml
    - requests
    - huggingface_hub

## Benchmark Runs
## -------------------------------------------------------------------------
## Define benchmark runs to execute on the remote server.
##
runs:
  - # -- Unique name for this benchmark run
    name: "remote-llama-benchmark"
    # -- Inference engine to use
    engine: "vllm"

    ## Server Configuration
    ## -----------------------------------------------------------------------
    serve:
      # -- Model path on the remote server
      model_path: "/data/models/Llama-3.1-8B-Instruct"
      # -- Server port
      port: 8000
      # -- GPU memory utilization (0.0-1.0)
      gpu_memory_utilization: 0.9
      # -- Maximum sequence length
      max_model_len: 8192
      # -- Maximum concurrent sequences
      max_num_seqs: 256
      # -- Data type for model weights
      dtype: "bfloat16"
      # -- Disable request logging
      disable_log_requests: true
      # -- Enable expert parallelism (for MoE models)
      enable_expert_parallel: false

      ## Parallelism Configurations
      ## ---------------------------------------------------------------------
      tp_dp_pairs:
        - tp: 1    # Tensor parallelism
          dp: 1    # Data parallelism
          pp: 1    # Pipeline parallelism

    ## Benchmark Configuration
    ## -----------------------------------------------------------------------
    bench:
      # -- Save results to JSON files
      save_results: false
      # -- Output directory
      output_dir: "./benchmark_results"
      # -- Context sizes to test
      context_size: [1024, 2048, 4096]
      # -- Concurrency levels
      concurrency: [50, 100]
      # -- Number of prompts per test
      num_prompts: [100]
      # -- Output token lengths
      output_len: [128]
